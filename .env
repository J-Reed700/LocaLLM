# Server Settings
HOST=0.0.0.0
PORT=8080
DEBUG=False
SECRET_KEY=your-secret-key-here

# Model Settings
MODEL_TYPE=text
MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
ENABLE_LLM_SERVICE=True
MODEL_RESOURCES_MAX_MEMORY=4GB
MODEL_RESOURCES_CPU_THREADS=4
MODEL_RESOURCES_DEVICE=cpu
MODEL_RESOURCES_PRECISION=fp16
MODEL_RESOURCES_CONTEXT_LENGTH=2048
MODEL_RESOURCES_BATCH_SIZE=1

# Database Settings
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=locallm
POSTGRES_USER=locallm
POSTGRES_PASSWORD=localdev

# Redis Settings
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=

# Telemetry Settings
TELEMETRY_ENABLED=True
JAEGER_AGENT_HOST=localhost
JAEGER_AGENT_PORT=6831
OTEL_SERVICE_NAME=locaLLM_server
OTEL_TRACES_EXPORTER=otlp
OTEL_EXPORTER_OTLP_ENDPOINT=http://host.docker.internal:4318/v1/traces
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
OTEL_EXPORTER_OTLP_INSECURE=True

# Application Settings
MAX_WORKERS=4
CACHE_TTL=300
ENVIRONMENT=production
SERVICE_NAME=locaLLM
SERVICE_VERSION=1.1.0
LOG_LEVEL=INFO

# API Settings
API_V1_PREFIX=/api/v1
API_KEY_HEADER=X-API-Key
DEFAULT_API_KEY=